{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b00aa565-0c94-4d82-bd77-9c952fc91588",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read topics of S3 bucket into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe749cd5-0601-4a11-9d69-4c8d5a0cee71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# list of topic suffixes\n",
    "topics = [\".pin\", \".geo\", \".user\"]\n",
    "\n",
    "def read_topics_into_dataframe(topic):\n",
    "    # create path to topic files\n",
    "    file_path = f\"/mnt/pinterest_0ecac53030fd/topics/0ecac53030fd{topic}/partition=0/*.json\"\n",
    "    # specify file type\n",
    "    file_type = \"json\"\n",
    "    # Ask Spark to infer the schema\n",
    "    infer_schema = \"true\"\n",
    "    # load JSONs from mounted S3 bucket to Spark dataframe\n",
    "    df = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .load(file_path)\n",
    "    return df\n",
    "\n",
    "dataframes = {}\n",
    "for topic in topics:\n",
    "    # Remove the leading dot for the DataFrame name\n",
    "    df_name = topic[1:]\n",
    "    # Directly call the function and store the DataFrame in a dictionary\n",
    "    dataframes[df_name] = read_topics_into_dataframe(topic)\n",
    "    # Display the DataFrame using Databricks' display function\n",
    "    display(dataframes[df_name])\n",
    "\n",
    "# Define DataFrames to access\n",
    "df_pin = dataframes['pin']\n",
    "df_geo = dataframes['geo']\n",
    "df_user = dataframes['user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22dc522-3174-4615-bff3-339ba0419ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean df_pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d896ef-3823-43cd-aa11-41c6d2187b19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_nulls_to_dataframe_column(dataframe, column, value_to_replace):\n",
    "    '''Converts matched values in column of dataframe to null based on expression'''\n",
    "    dataframe = dataframe.withColumn(column, when(col(column).like(value_to_replace), None).otherwise(col(column)))\n",
    "    return dataframe\n",
    "\n",
    "# replace empty entries and entries with no relevant data in each column with Nones\n",
    "# column names and values to change to null\n",
    "columns_and_values_for_null = {\n",
    "    \"description\": \"No description available%\",\n",
    "    \"follower_count\": \"User Info Error\",\n",
    "    \"image_src\": \"Image src error.\",\n",
    "    \"poster_name\": \"User Info Error\",\n",
    "    \"tag_list\": \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\",\n",
    "    \"title\": \"No Title Data Available\"\n",
    "}\n",
    "\n",
    "# loop through dictionary, calling function with dictionary values as arguments\n",
    "for key, value in columns_and_values_for_null.items():\n",
    "    df_pin = add_nulls_to_dataframe_column(df_pin, key, value)\n",
    "# Perform the necessary transformations on the follower_count to ensure every entry is a number\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "# cast follower_count column to integer type\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast('int'))\n",
    "# convert save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "# rename the index column to ind\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "# reorder columns\n",
    "new_pin_column_order = [\n",
    "    \"ind\",\n",
    "    \"unique_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"follower_count\",\n",
    "    \"poster_name\",\n",
    "    \"tag_list\",\n",
    "    \"is_image_or_video\",\n",
    "    \"image_src\",\n",
    "    \"save_location\",\n",
    "    \"category\"\n",
    "]\n",
    "df_pin = df_pin.select(new_pin_column_order)\n",
    "\n",
    "# Display the final schema and data\n",
    "df_pin.printSchema()\n",
    "display(df_pin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "232dc141-d127-4f01-b071-0bd1a6c70e93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31b5824-081a-4719-a275-588fbd2882eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import types\n",
    "from pyspark.sql.functions import array, to_timestamp\n",
    "\n",
    "# Creates a new column \"coordinates\" that contains an array based on the latitude and longitude columns\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\")).drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# convert timestamp column from type string to type timestamp\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "# change column order\n",
    "new_geo_column_order = [\n",
    "    \"ind\",\n",
    "    \"country\",\n",
    "    \"coordinates\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "df_geo = df_geo.select(new_geo_column_order)  \n",
    "\n",
    "# display changes\n",
    "df_geo.printSchema()\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c34e0e-d917-41a8-8e60-f219aa0f50ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c77b2fa8-ceef-40d6-b547-30ce5057a03a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, to_timestamp\n",
    "\n",
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns and Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.withColumn(\"user_name\", concat(\"first_name\",lit( \" \"), \"last_name\")).drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "# Reorder the DataFrame columns to have the following column order\n",
    "new_user_column_order = [\n",
    "    \"ind\",\n",
    "    \"user_name\",\n",
    "    \"age\",\n",
    "    \"date_joined\",\n",
    "]\n",
    "df_user = df_user.select(new_user_column_order) \n",
    "# display changes\n",
    "df_user.printSchema()\n",
    "display(df_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d61cd9-1bf9-4eb9-b3a0-a4852678e5dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preparing for Queries ( joins, age groups )\n",
    "# import window functions and col \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# join df_pin and df_geo dataframes on index\n",
    "pin_geo = df_pin.alias(\"pin\").join(df_geo.alias(\"geo\"), col(\"pin.ind\") == col(\"geo.ind\"), \"inner\")\n",
    "\n",
    "# join df_pin and df_user and create temp view for SQL query\n",
    "df_pin.alias(\"pin\").join(df_user.alias(\"user\"), col(\"pin.ind\") == col(\"user.ind\")).createOrReplaceTempView(\"category_age\")\n",
    "# SQL query to create age group column\n",
    "pin_user_age_group = spark.sql(\n",
    "    \"SELECT CASE \\\n",
    "        WHEN age between 18 and 24 then '18-24' \\\n",
    "        WHEN age between 25 and 35 then '25-35' \\\n",
    "        WHEN age between 36 and 50 then '36-50' \\\n",
    "        WHEN age > 50 then '50+' \\\n",
    "        END as age_group, * FROM category_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78deb24-ba70-411d-9200-777f6f537aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Task4 : Find the most popular category in each country.\n",
    "# creates partition by country and order by category_count descending\n",
    "windowCountryByCategoryCount = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Calculates the most popular category for each country\n",
    "pin_geo \\\n",
    ".groupBy(\"country\", \"category\") \\\n",
    ".agg(count(\"category\").alias(\"category_count\")) \\\n",
    ".withColumn(\"rank\", row_number().over(windowCountryByCategoryCount)) \\\n",
    ".filter(col(\"rank\") == 1) \\\n",
    ".drop(\"rank\") \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ed5fcf-312f-4937-9e45-816674e6235a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 5: Find which was the most popular category each year\n",
    "from pyspark.sql.functions import col, year, count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# creates partition by year and order by category_count descending \n",
    "windowYearByCategoryCount = Window.partitionBy(\"post_year\").orderBy(col(\"category_count\").desc())\n",
    "# finds which was the most popular category each year between 2018 and 2022\n",
    "pin_geo.withColumn(\"post_year\", year(\"timestamp\")) \\\n",
    ".filter(col(\"post_year\") >= 2018) \\\n",
    ".filter(col(\"post_year\") <= 2022) \\\n",
    ".groupBy(\"post_year\", \"category\") \\\n",
    ".agg(count(\"category\").alias(\"category_count\")) \\\n",
    ".withColumn(\"rank\", row_number().over(windowYearByCategoryCount)) \\\n",
    ".filter(col(\"rank\") == 1) \\\n",
    ".drop(\"rank\") \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52075e57-b898-4f6e-8b23-1be54eead53f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 6: Find the user with most followers in each country \n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# creates partition by country and order by follower_count descending\n",
    "windowCountryByFollowers = Window.partitionBy(\"country\").orderBy(col(\"follower_count\").desc())\n",
    "\n",
    "# finds the user with the most followers in each country\n",
    "max_followers_by_country = \\\n",
    " pin_geo \\\n",
    " .withColumn(\"rank\", row_number().over(windowCountryByFollowers)) \\\n",
    " .filter(col(\"rank\") == 1) \\\n",
    " .select(\"country\", \"poster_name\", \"follower_count\") \\\n",
    " \n",
    "# gets highest number of followers from all countries\n",
    "max_followers_all_countries = max_followers_by_country.select(max(\"follower_count\")).collect()[0][0]\n",
    "\n",
    "# finds the country with the user with most followers\n",
    "country_with_max_followers = \\\n",
    "    max_followers_by_country \\\n",
    "    .select(\"*\") \\\n",
    "    .where(col(\"follower_count\") == max_followers_all_countries)\n",
    "\n",
    "max_followers_by_country.show()\n",
    "country_with_max_followers.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7380fb8-04e1-45aa-8e3e-ad985052bdf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 7: Find the most popular category for different age groups\n",
    "# create partition by age_group and order by category_count descending\n",
    "windowAgeGroupByCategory = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "# find the most popular category for different age groups\n",
    "pin_user_age_group \\\n",
    ".groupBy(\"age_group\", \"category\") \\\n",
    ".agg(count(\"category\").alias(\"category_count\")) \\\n",
    ".withColumn(\"rank\", row_number().over(windowAgeGroupByCategory)) \\\n",
    ".filter(col(\"rank\") == 1) \\\n",
    ".drop(\"rank\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ddd3dd-460e-463c-be16-a748cb03ee29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 8: Find  the median follower count for different age groups\n",
    "# create partition by age_group and order by category_count descending\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "# Assuming pin_user_age_group is a DataFrame that already has 'age_group' and 'follower_count' defined\n",
    "# Find the median follower count for different age groups\n",
    "pin_user_age_group \\\n",
    "    .select(\"age_group\", \"follower_count\") \\\n",
    "    .distinct() \\\n",
    "    .groupBy(\"age_group\") \\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .orderBy(\"age_group\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e0a7d9-9468-451e-aba5-fe26ce9fd7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 9: How many users have joined each year\n",
    "# Find how many users have joined between 2015 and 2020.\n",
    "df_user \\\n",
    ".withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    ".distinct() \\\n",
    ".filter((year('date_joined') >= 2015) & (year('date_joined') <= 2020)) \\\n",
    ".groupBy(\"post_year\") \\\n",
    ".agg(count(\"user_name\").alias(\"number_users_joined\")) \\\n",
    ".orderBy(\"post_year\") \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc456b8-7cfb-47d3-a55a-f991d8ca9c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, year, percentile_approx\n",
    "# Task 10 : Find the median follower count of users have joined between 2015 and 2020.\n",
    "\n",
    "# Perform the join and create a new column for the post year based on the date joined\n",
    "pin_user = df_pin.alias(\"pin\").join(df_user.alias(\"user\"), col(\"pin.ind\") == col(\"user.ind\"),\"inner\") \\\n",
    "\n",
    "pin_user \\\n",
    ".withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    ".filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020)) \\\n",
    ".groupBy(\"post_year\") \\\n",
    ".agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    ".orderBy(\"post_year\") \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cf8e23-725f-435f-ac48-51a71bb18422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 11: Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of.\n",
    "from pyspark.sql.functions import col, year, percentile_approx\n",
    "pin_user_age_group \\\n",
    ".withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    ".filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020)) \\\n",
    ".groupBy(\"age_group\", \"post_year\") \\\n",
    ".agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    ".orderBy(\"age_group\",\"post_year\") \\\n",
    ".show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clean_and_queries_batch_data.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
